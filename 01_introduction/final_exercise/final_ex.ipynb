{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Compose several transformers together\n",
    "transforms.Compose(transformers)\n",
    "\n",
    "# Convert a PIL Image or numpy.ndarray to tensor\n",
    "# Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "transforms.ToTensor()\n",
    "\n",
    "# Normalize a tensor image with mean and standard deviation\n",
    "transforms.Normalize(mean, std, inplace=False)\n",
    "transforms.Normalize((0.5,),(0.5,))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist function is for the purpose of loading mnist data-set\n",
    "def mnist():\n",
    "    # Define a transform to normalize the data\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,)),\n",
    "                                   ])\n",
    "    \n",
    "    trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "    train = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    testset = datasets.FashionMNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "    test = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch size of each image: torch.Size([64, 1, 28, 28])\n",
      "label: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# An example in mnist data-set\n",
    "\n",
    "dataiter = iter(train)\n",
    "images, labels = dataiter.next()\n",
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');\n",
    "print(\"torch size of each image: {}\".format(images.shape))\n",
    "print(\"label: {}\".format(labels[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyAwesomeModel(\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=490, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=172, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyAwesomeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, input_height, input_width, num_classes, num_zoom=3):\n",
    "        super(MyAwesomeModel, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "        self.num_classes = num_classes\n",
    "        self.num_zoom = num_zoom\n",
    "        \n",
    "        # Spatial transformer localization-network\n",
    "        # nn.Sequential http://pytorch.org/docs/master/nn.html#torch.nn.Sequential\n",
    "        #   A sequential container. \n",
    "        #   Modules will be added to it in the order they are passed in the constructor.\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels,\n",
    "                      out_channels=8, \n",
    "                      kernel_size=7, \n",
    "                      padding=3),\n",
    "            nn.MaxPool2d(kernel_size=2, \n",
    "                         stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=8, \n",
    "                      out_channels=10, \n",
    "                      kernel_size=5, \n",
    "                      padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix that we use \n",
    "        # to make the bilinear interpolation for the spatial transformer\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(in_features=10 * input_height//4 * input_width//4, \n",
    "                      out_features=32,\n",
    "                      bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=32, \n",
    "                      out_features=3 * 2,\n",
    "                      bias=True)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        # see the article for a definition and explanation for this\n",
    "        self.fc_loc[2].weight.data.fill_(0)\n",
    "        self.fc_loc[2].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])\n",
    "        \n",
    "        # The classification network based on the transformed (cropped) image\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels,\n",
    "                               out_channels=16,\n",
    "                               kernel_size=5,\n",
    "                               padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=5,\n",
    "                               padding=2)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        \n",
    "        # fully connected output layers\n",
    "        self.fc1_features = 32 * input_height//num_zoom//4 * input_width//num_zoom//4\n",
    "        self.fc1 = nn.Linear(in_features=self.fc1_features, \n",
    "                             out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50,\n",
    "                             out_features=num_classes)\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        \"\"\" Spatial Transformer Network \"\"\"\n",
    "        # creates distributed embeddings of the image with the location network.\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * self.input_height//4 * self.input_width//4)\n",
    "        # project from distributed embeddings to bilinear interpolation space\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        \n",
    "        # define the output size of the cropped tensor\n",
    "        # notice that we divide the height and width with the amount of zoom\n",
    "        output_size = torch.Size((x.size()[0],\n",
    "                                  x.size()[1], \n",
    "                                  x.size()[2]//self.num_zoom,\n",
    "                                  x.size()[3]//self.num_zoom))\n",
    "        # magic pytorch functions that are used for transformer networks\n",
    "        grid = F.affine_grid(theta, output_size) # http://pytorch.org/docs/master/nn.html#torch.nn.functional.affine_grid\n",
    "        x = F.grid_sample(x, grid) # http://pytorch.org/docs/master/nn.html#torch.nn.functional.grid_sample\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = self.stn(x)\n",
    "        # save transformation\n",
    "        l_trans1 = Variable(x.data)\n",
    "\n",
    "        # Perform the usual forward pass\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, self.fc1_features)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # note use of Functional.dropout, where training must be explicitly defined (default: False)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        # return output and batch of bilinear interpolated images\n",
    "        return F.log_softmax(x, dim=1), l_trans1\n",
    "\n",
    "\n",
    "model = MyAwesomeModel(1, 28, 28, 10)\n",
    "if torch.cuda.is_available():\n",
    "    print('##converting network to cuda-enabled')\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainOREvaluate(object):\n",
    "    \"\"\" Helper class that will help launch class methods as commands\n",
    "        from a single script\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        parser = argparse.ArgumentParser(\n",
    "            description=\"Script for either training or evaluating\",\n",
    "            usage=\"python main.py <command>\"\n",
    "        )\n",
    "        parser.add_argument(\"command\", help=\"Subcommand to run\")\n",
    "        args = parser.parse_args(sys.argv[1:2])\n",
    "        if not hasattr(self, args.command):\n",
    "            print('Unrecognized command')\n",
    "            \n",
    "            parser.print_help()\n",
    "            exit(1)\n",
    "        # use dispatch pattern to invoke method with same name\n",
    "        getattr(self, args.command)()\n",
    "    \n",
    "    def train(self):\n",
    "        print(\"Training day and night\")\n",
    "        parser = argparse.ArgumentParser(description='Training arguments')\n",
    "        parser.add_argument('--lr', default=0.1)\n",
    "        # add any additional argument that you want\n",
    "        args = parser.parse_args(sys.argv[2:])\n",
    "        print(args)\n",
    "        \n",
    "        # Implement training loop \n",
    "        model = MyAwesomeModel()\n",
    "        train_set, _ = mnist()\n",
    "        \n",
    "        epochs = 30\n",
    "        steps = 0\n",
    "        train_losses = []\n",
    "        for e in range(epochs):\n",
    "            running_loss = 0\n",
    "            for images, labels in train_set:\n",
    "                optimizer.zero_grad()\n",
    "                log_ps = model(images)\n",
    "                loss = criterion(log_ps, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print (running_loss)\n",
    "        \n",
    "        \n",
    "    def evaluate(self):\n",
    "        print(\"Evaluating until hitting the ceiling\")\n",
    "        parser = argparse.ArgumentParser(description='Training arguments')\n",
    "        parser.add_argument('--load_model_from', default=\"\")\n",
    "        # add any additional argument that you want\n",
    "        args = parser.parse_args(sys.argv[2:])\n",
    "        print(args)\n",
    "        \n",
    "        # TODO: Implement evaluation logic here\n",
    "        if args.load_model_from:\n",
    "            model = torch.load(args.load_model_from)\n",
    "        _, test_set = mnist()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TrainOREvaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
